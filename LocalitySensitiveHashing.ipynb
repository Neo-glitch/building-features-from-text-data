{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enables hashing text close together in the same hashing bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasketch in d:\\anaconda_3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.11 in d:\\anaconda_3\\lib\\site-packages (from datasketch) (1.19.2)\n"
     ]
    }
   ],
   "source": [
    "# 11th\n",
    "\n",
    "# needed to get prob structures needed to process and search large datset fast and accurately\n",
    "!pip install datasketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHash, MinHashLSH  # minHashLSH: min hash locallity sensitive hashing\n",
    "\n",
    "from nltk import ngrams\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_array = [\"A bird in hand is worth two in the bush.\",\n",
    "       \"Good things come to those who hustle.\",\n",
    "       \"There are other fish in the sea.\",\n",
    "       \"The ball is in your court.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'bird', 'in', 'hand', 'is', 'worth', 'two', 'in', 'the', 'bush', '.'],\n",
       " ['Good', 'things', 'come', 'to', 'those', 'who', 'hustle', '.'],\n",
       " ['There', 'are', 'other', 'fish', 'in', 'the', 'sea', '.'],\n",
       " ['The', 'ball', 'is', 'in', 'your', 'court', '.']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1:\n",
    "word_token_array = [word_tokenize(text) for text in text_array]\n",
    "\n",
    "word_token_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('A', 'bird', 'in')\n",
      "0 ('bird', 'in', 'hand')\n",
      "0 ('in', 'hand', 'is')\n",
      "0 ('hand', 'is', 'worth')\n",
      "0 ('is', 'worth', 'two')\n",
      "0 ('worth', 'two', 'in')\n",
      "0 ('two', 'in', 'the')\n",
      "0 ('in', 'the', 'bush')\n",
      "0 ('the', 'bush', '.')\n",
      "1 ('Good', 'things', 'come')\n",
      "1 ('things', 'come', 'to')\n",
      "1 ('come', 'to', 'those')\n",
      "1 ('to', 'those', 'who')\n",
      "1 ('those', 'who', 'hustle')\n",
      "1 ('who', 'hustle', '.')\n",
      "2 ('There', 'are', 'other')\n",
      "2 ('are', 'other', 'fish')\n",
      "2 ('other', 'fish', 'in')\n",
      "2 ('fish', 'in', 'the')\n",
      "2 ('in', 'the', 'sea')\n",
      "2 ('the', 'sea', '.')\n",
      "3 ('The', 'ball', 'is')\n",
      "3 ('ball', 'is', 'in')\n",
      "3 ('is', 'in', 'your')\n",
      "3 ('in', 'your', 'court')\n",
      "3 ('your', 'court', '.')\n"
     ]
    }
   ],
   "source": [
    "# step 2: shingling(gen n_grams of words)\n",
    "\n",
    "for index, word_tokens in enumerate(word_token_array):\n",
    "    for n_gram in ngrams(word_tokens, 3):  # gen tri gram(i n practice 8 - 10 it 9ice value of n for process)\n",
    "        print(index, n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: locality sensitive hashing\n",
    "min_hash_lsh= MinHashLSH(threshold = 0.5, # thresh is based on jaccard index value\n",
    "                         num_perm=128)  # perm increases hashing accuracy, but preformance will be slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: calc min hash for every shingle in input text\n",
    "min_hashes = {}\n",
    "\n",
    "for index, text in enumerate(text_array):  # create a min hash obj for every sen in text_array\n",
    "    min_hash = MinHash(num_perm=128)\n",
    "    \n",
    "    for n_gram in ngrams(text, 3):  # gen n_grams for each input text sen\n",
    "        min_hash.update(\"\".join(n_gram).encode(\"utf-8\"))  # updates min hash for a sentence with min hash shingle in focus\n",
    "        \n",
    "    min_hash_lsh.insert(index, min_hash)  # feed to lsh obj\n",
    "    min_hashes[index] = min_hash\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <datasketch.minhash.MinHash at 0x1f71a091df0>,\n",
       " 1: <datasketch.minhash.MinHash at 0x1f71a06d760>,\n",
       " 2: <datasketch.minhash.MinHash at 0x1f7196ac1f0>,\n",
       " 3: <datasketch.minhash.MinHash at 0x1f7196b7310>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate tri gram with jaccard sim index > 0.5 for input 0 ; [0]\n",
      "Candidate tri gram with jaccard sim index > 0.5 for input 1 ; [1]\n",
      "Candidate tri gram with jaccard sim index > 0.5 for input 2 ; [2]\n",
      "Candidate tri gram with jaccard sim index > 0.5 for input 3 ; [3]\n"
     ]
    }
   ],
   "source": [
    "# ites through all min hashes for input text\n",
    "for i in min_hashes.keys():\n",
    "    result = min_hash_lsh.query(min_hashes[i])  # query for similar doc\n",
    "    print(\"Candidate tri gram with jaccard sim index > 0.5 for input\", i, \";\", result)\n",
    "    \n",
    "    # shows sentence is similar to only setence one and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using more similar text_data\n",
    "text_array = [\"A bird in the hand is worth two in the bush.\",\n",
    "             \"A bird in hands is worth three in the bushes.\",\n",
    "             \"Good things come to those who wait.\",\n",
    "             \"Good tpings cxme to those who wait long.\",\n",
    "             \"There are other fish in the sea.\",\n",
    "             \"The ball is in your court\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh is 0.5, so points of jaca index similarity of 0.5 and higher suppose to belong to same hash bucket.\n",
    "\n",
    "min_hash_lsh = MinHashLSH(threshold = 0.5, num_perm=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: calc min hash for every shingle in input text\n",
    "min_hashes = {}\n",
    "\n",
    "for index, text in enumerate(text_array):  # create a min hash obj for every sen in text_array\n",
    "    min_hash = MinHash(num_perm=128)\n",
    "    \n",
    "    for n_gram in ngrams(text, 3):  # gen n_grams for each input text sen\n",
    "        min_hash.update(\"\".join(n_gram).encode(\"utf-8\"))  # updates min hash for a sentence with min hash shingle in focus\n",
    "        \n",
    "    min_hash_lsh.insert(index, min_hash)  # feed to lsh obj\n",
    "    min_hashes[index] = min_hash\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate tri gram with jaccard sim index > 0.5 for input 0 ; [0, 1]\n",
      "Candidate tri gram with jaccard sim index > 0.5 for input 1 ; [0, 1]\n",
      "Candidate tri gram with jaccard sim index > 0.5 for input 2 ; [2, 3]\n",
      "Candidate tri gram with jaccard sim index > 0.5 for input 3 ; [2, 3]\n",
      "Candidate tri gram with jaccard sim index > 0.5 for input 4 ; [4]\n",
      "Candidate tri gram with jaccard sim index > 0.5 for input 5 ; [5]\n"
     ]
    }
   ],
   "source": [
    "# ites through all min hashes for input text\n",
    "for i in min_hashes.keys():\n",
    "    result = min_hash_lsh.query(min_hashes[i])  # query for similar doc\n",
    "    print(\"Candidate tri gram with jaccard sim index > 0.5 for input\", i, \";\", result)\n",
    "    \n",
    "    # shows sentence 0 is similar to 0 and 1 and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
